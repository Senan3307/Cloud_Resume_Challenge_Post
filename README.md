# Cloud_Resume_Challenge_Post

# The What and Why of the Cloud Resume Challenge 

The Cloud Resume Challenge was an incredible project that gave me hands-on experience with cloud computing. I created and hosted my personal resume on AWS, using HTML, CSS, and JavaScript, while diving into various cloud technologies. Some key steps included setting up a custom domain with HTTPS, building a visitor counter with a back-end API, and storing visitor data in a database. I also managed the infrastructure using Terraform and implemented a CI/CD pipeline for automated testing and deployment. Throughout the process, I documented my work in a GitHub repository, which helped me organize my project and showcase my progress.

For me, the challenge was an excellent way to learn and demonstrate cloud computing skills. It provided hands-on experience with hosting, APIs, databases, and version control, all while teaching me problem-solving and best practices. Beyond the technical skills, completing the project gave me something that I can add onto my resume that highlights my initiative and technical abilities to potential employers. By the end, I felt more confident in my cloud skills and my ability to deploy various tools on AWS adequately. 

# Services Used and the purpose for each service

At the project's core, the S3 (Simple Storage Service) bucket served as the foundation, hosting static website content such as HTML, CSS, and JavaScript files. These static files were seamlessly delivered to end users through CloudFront, a content delivery network that cached website content for global distribution, ensuring low-latency access and high availability. Alongside CloudFront, AWS Certificate Manager enabled secure HTTPS connections, bolstering the website's credibility and security.

To implement backend functionality like tracking website visits, the project utilized AWS Lambda for serverless logic. The Lambda function was triggered through API Gateway, which served as the secure API endpoint, exposing the function to the internet. The visitor data was then stored in DynamoDB, a managed NoSQL database ideal for handling the simple key-value pairs required for this task. This integration ensured that the system was entirely serverless and could handle high request volumes without the need for manual database management. IAM (Identity and Access Management) policies were critical for securing these resources, providing precise permissions to Lambda, S3, and DynamoDB to ensure only authorized actions were performed.

The custom domain, senanotsuka.com, was managed by Route 53, which handled DNS configuration and routed traffic to the S3 bucket or CloudFront distribution. This streamlined the user experience by providing an easy-to-remember domain name while ensuring seamless integration with other AWS services. Although EC2 was introduced during the learning process, it was deliberately excluded from the project to maintain the serverless architecture's efficiency and cost-effectiveness. Together, these services formed a cohesive, scalable architecture where each component interacted seamlessly to deliver a modern, functional resume website.

# What I learned through the Cloud Resume Challenge

The first important skill I learned through the Cloud Resume Challenge was the importance of good identity and access management (IAM) practices on AWS. The root user on AWS holds a lot of power, and if someone gains access to your root credentials, they could use your account to perform extremely expensive tasks. Furthermore, if your AWS account contains sensitive or important information, root access could allow unauthorized users to exploit various tools on AWS, causing a significant security breach. Thus, one of the first things we did was use IAM to manage roles and policies. This ensured that we didn’t need to use the root account when accessing AWS resources. We essentially applied the principle of least privilege, starting by assigning only the minimum permissions required.

Additionally, when linking AWS to tools such as VS Code, we had to grant the terminal access to AWS through the AWS Command Line Interface (CLI). One of the most critical practices was to avoid inputting user credentials directly into the AWS CLI, as doing so could expose the credentials to anyone with access to the machine. Instead, we used a Key Pair to establish a secure way to access AWS through the VS Code terminal. Overall, this project taught me the importance of strong security practices through effective access management by understanding who has access to what and ensuring the least privilege principle is upheld.

Another important skill I learned during this project was automation using a CI/CD pipeline. At the beginning of the project, I frequently had to access the AWS Management Console, navigating through multiple services and manually configuring settings to enable specific processes. However, through automation using CI/CD actions and Infrastructure as Code (IaC), I no longer needed to log into the console for these tasks. Instead, I could configure most things through my terminal in VS Code, saving time and reducing human error.

Terraform was used for IaC, which streamlined the deployment, testing, and management processes. For example, if I wanted to redeploy the entire project from scratch, it would be much faster since the infrastructure was already defined in code—I would only need to rerun the Terraform script. Additionally, having the infrastructure as code made it easier to conduct unit, system, and end-to-end tests, allowing us to identify and resolve infrastructure issues more effectively.

The CI/CD pipeline was implemented using GitHub and GitHub Actions. This automation pipeline allowed us to detect problems with the deployment process quickly, providing a more reliable and consistent delivery mechanism. By incorporating CI/CD, we achieved more efficient and error-free deployment workflows, enhancing both productivity and reliability throughout the project.

